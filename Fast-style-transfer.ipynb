{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FSF98p7H81ag"
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import sys, os\n",
    "import numpy as np, scipy.misc\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import scipy.io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5HbVCfGWJU-6"
   },
   "outputs": [],
   "source": [
    "def loss_net(image):\n",
    "    conv1 = _conv_layer(image, 32, 9, 1)\n",
    "    conv2 = _conv_layer(conv1, 64, 3, 2)\n",
    "    conv3 = _conv_layer(conv2, 128, 3, 2)\n",
    "    resid1 = _residual_block(conv3, 3)\n",
    "    resid2 = _residual_block(resid1, 3)\n",
    "    resid3 = _residual_block(resid2, 3)\n",
    "    resid4 = _residual_block(resid3, 3)\n",
    "    resid5 = _residual_block(resid4, 3)\n",
    "    conv_t1 = _conv_tranpose_layer(resid5, 64, 3, 2)\n",
    "    conv_t2 = _conv_tranpose_layer(conv_t1, 32, 3, 2)\n",
    "    conv_t3 = _conv_layer(conv_t2, 3, 9, 1, relu=False)\n",
    "    preds = tf.nn.tanh(conv_t3) * 150 + 255./2\n",
    "    return preds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FgD0kyseHgl5"
   },
   "outputs": [],
   "source": [
    "# https://github.com/anishathalye/neural-style\n",
    "\n",
    "mean_pixel_rgb = np.array([ 123.68 ,  116.779,  103.939])\n",
    "\n",
    "def net(data_path, input_image):\n",
    "    layers = (\n",
    "        'conv1_1', 'relu1_1', 'conv1_2', 'relu1_2', 'pool1',\n",
    "\n",
    "        'conv2_1', 'relu2_1', 'conv2_2', 'relu2_2', 'pool2',\n",
    "\n",
    "        'conv3_1', 'relu3_1', 'conv3_2', 'relu3_2', 'conv3_3',\n",
    "        'relu3_3', 'conv3_4', 'relu3_4', 'pool3',\n",
    "\n",
    "        'conv4_1', 'relu4_1', 'conv4_2', 'relu4_2', 'conv4_3',\n",
    "        'relu4_3', 'conv4_4', 'relu4_4', 'pool4',\n",
    "\n",
    "        'conv5_1', 'relu5_1', 'conv5_2', 'relu5_2', 'conv5_3',\n",
    "        'relu5_3', 'conv5_4', 'relu5_4'\n",
    "    )\n",
    "\n",
    "    data = scipy.io.loadmat(data_path)\n",
    "    mean = data['normalization'][0][0][0]\n",
    "    mean_pixel = np.mean(mean, axis=(0, 1))\n",
    "    weights = data['layers'][0]\n",
    "\n",
    "    net = {}\n",
    "    current = input_image\n",
    "    for i, name in enumerate(layers):\n",
    "        kind = name[:4]\n",
    "        if kind == 'conv':\n",
    "            kernels, bias = weights[i][0][0][0][0]\n",
    "            # matconvnet: weights are [width, height, in_channels, out_channels]\n",
    "            # tensorflow: weights are [height, width, in_channels, out_channels]\n",
    "            kernels = np.transpose(kernels, (1, 0, 2, 3))\n",
    "            bias = bias.reshape(-1)\n",
    "            current = _conv_layer(current, kernels, bias)\n",
    "        elif kind == 'relu':\n",
    "            current = tf.nn.relu(current)\n",
    "        elif kind == 'pool':\n",
    "            current = _pool_layer(current)\n",
    "        net[name] = current\n",
    "\n",
    "    assert len(net) == len(layers)\n",
    "    return net\n",
    "\n",
    "\n",
    "def _conv_layer(input, weights, bias):\n",
    "    conv = tf.nn.conv2d(input, tf.constant(weights), strides=(1, 1, 1, 1),\n",
    "            padding='SAME')\n",
    "    return tf.nn.bias_add(conv, bias)\n",
    "\n",
    "\n",
    "def _pool_layer(input):\n",
    "    return tf.nn.max_pool(input, ksize=(1, 2, 2, 1), strides=(1, 2, 2, 1),\n",
    "            padding='SAME')\n",
    "\n",
    "\n",
    "def preprocess(image):\n",
    "    return image - mean_pixel_rgb\n",
    "\n",
    "\n",
    "def unprocess(image):\n",
    "    return image + mean_pixel_rgb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gram_matrix(input_tensor):  \n",
    "  # from my implementation of style transfer optimization!\n",
    "  channels = int(input_tensor.shape[-1])\n",
    "  a = tf.reshape(input_tensor, [-1, channels])\n",
    "  n = tf.shape(a)[0]\n",
    "  # Dot product of every vector being done here\n",
    "  gram = tf.matmul(a, a, transpose_a=True)\n",
    "  return gram / tf.cast(n, tf.float32)\n",
    "\n",
    "def get_style_loss(base_style, gram_target):\n",
    "  \n",
    "  height, width, channels = base_style.get_shape().as_list()\n",
    "  gram_style = gram_matrix(base_style)\n",
    "  # You then subtract gram matrices to get style loss!\n",
    "  return tf.reduce_mean(tf.square(gram_style - gram_target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MjuIV5Ws9XWR"
   },
   "outputs": [],
   "source": [
    "content_weight = 7.5e0\n",
    "style_weight = 1e2\n",
    "tv_weight = 2e2\n",
    "\n",
    "learning_rate = 1e-3\n",
    "epochs = 2\n",
    "# checkpoint_dir = 'checkpoints'\n",
    "# CHECKPOINT_ITERATIONS = 2000\n",
    "vgg_path = 'data/imagenet-vgg-verydeep-19.mat'\n",
    "dataset_path = 'data/train2014'\n",
    "batch_size = 4\n",
    "iterations = 40,000\n",
    "style_path = 'the-scream.jpg'\n",
    "\n",
    "files = os.listdir(dataset_path)\n",
    "content_targets = [os.path.join(img_dir,x) for x in files]\n",
    "\n",
    "style_target = resize_image(options.style)\n",
    "\n",
    "# optimize..."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Untitled5.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
